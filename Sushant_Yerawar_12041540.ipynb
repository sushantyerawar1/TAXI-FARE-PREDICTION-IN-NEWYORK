{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c0449e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Part 1: New York City Taxi Fare Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be33511e",
   "metadata": {},
   "source": [
    "# 1.Data Cleaning and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "413dbf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am importing inbuilt libraries which are necessary for Data Cleaning and Visualization \n",
    "\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pycountry\n",
    "from statistics import mean, stdev\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "from scipy.stats import kstest\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import statsmodels.api as sm\n",
    "import plotly.express as px\n",
    "from scipy.stats import kstest\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc59dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data set in train dataset which have 1 lakh entries\n",
    "train = pd.read_csv(\"Desktop/DATASET/train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d03989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c3cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check no of rows & columns in the dataset\n",
    "print(\"Shape of the Training data :\", train.shape)\n",
    "\n",
    "# See all the columns in the dataset\n",
    "print(\"All columns are :\", list(train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63484519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check datatypes & count of not-null values in each field\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749514dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values in train data\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12156cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the missing values\n",
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5160e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['key'] = pd.to_datetime(train['key'])\n",
    "train['pickup_datetime']  = pd.to_datetime(train['pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be8506",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [train]\n",
    "for i in data:\n",
    "    i['Year'] = i['pickup_datetime'].dt.year\n",
    "    i['Month'] = i['pickup_datetime'].dt.month\n",
    "    i['Date'] = i['pickup_datetime'].dt.day\n",
    "    i['Day of Week'] = i['pickup_datetime'].dt.dayofweek\n",
    "    i['Hour'] = i['pickup_datetime'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb8ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decscribe() calculate number of rows and also calculate mean,std of all entries of column and also \n",
    "# find min and max value from the columns \n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a670df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am droping 'Unnamed: 0' column \n",
    "df = pd.DataFrame(train, columns=['pickup_datetime','pickup_longitude', 'pickup_latitude', \n",
    "                                 'dropoff_longitude', 'dropoff_latitude','passenger_count','fare_amount','Year', 'Month', 'Date',\n",
    "       'Day of Week', 'Hour'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b5811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latitudes range from -90 to 90.\n",
    "# Longitudes range from -180 to 180.\n",
    "# droping irrelevant entries of Latitudes,Longitudes  \n",
    "df = df[df['pickup_latitude']>=-90]\n",
    "df = df[df['pickup_latitude']<=90]\n",
    "df = df[df['dropoff_latitude']>=-90]\n",
    "df = df[df['dropoff_latitude']<=90]\n",
    "df = df[df['pickup_longitude']>=-180]\n",
    "df = df[df['pickup_longitude']<=180]\n",
    "df = df[df['dropoff_longitude']>=-180]\n",
    "df = df[df['dropoff_longitude']<=180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4fe5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using haversine_distance function to calculate distance between pickup point to droping point\n",
    "# from given pickup_latitude,pickup_longitude,dropoff_latitude,dropoff_longitude\n",
    "\n",
    "def haversine_distance(lat1, long1, lat2, long2):\n",
    "    data = [df]\n",
    "    for i in data:\n",
    "        R = 6371  #radius of earth in kilometers\n",
    "        phi1 = np.radians(i[lat1])\n",
    "        phi2 = np.radians(i[lat2])\n",
    "    \n",
    "        delta_phi = np.radians(i[lat2]-i[lat1])\n",
    "        delta_lambda = np.radians(i[long2]-i[long1])\n",
    "    \n",
    "        #a = sin²((φB - φA)/2) + cos φA . cos φB . sin²((λB - λA)/2)\n",
    "        a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0) ** 2\n",
    "    \n",
    "        #c = 2 * atan2( √a, √(1−a) )\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    \n",
    "        #d = R*c\n",
    "        d = (R * c) #in kilometers\n",
    "        i['H_Distance'] = d\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f35c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling haversine_distance function which is declare above\n",
    "distance = haversine_distance('pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is output after adding H_Distance\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4acbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude' and 'pickup_datetime' columns from df dataframe\n",
    "# because they are mandatory for further work\n",
    "\n",
    "del df['pickup_datetime']\n",
    "del df['pickup_longitude']\n",
    "del df['pickup_latitude']\n",
    "del df['dropoff_longitude']\n",
    "del df['dropoff_latitude']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add6f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for no. of non-positive fare_amount values.\n",
    "df[df['fare_amount']<=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9920ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping all rows having negative H_Distance(meaningless data) or having negative fare_amount or having negative passenger_count\n",
    "df = df[df['H_Distance']>0]\n",
    "df = df[df['fare_amount']>0]\n",
    "df = df[df['passenger_count']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a0cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d087e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am adding new column fare_amount_per_KM\n",
    "# and it is function to calculate fare_amount_per_KM for each entry \n",
    "def fare_amount_per_KM(col1, col2):\n",
    "    data = [df]\n",
    "    for i in data:\n",
    "        ans = i[col1]/i[col2]\n",
    "        i['Fare_Amount_Per_KM'] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da77d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling fare_amount_per_KM function\n",
    "fare_amount_per_KM('fare_amount','H_Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df dataframe after adding fare_amount_per_KM column\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7017de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between columns \n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e6552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11769726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing outliers with the help of plots\n",
    "\n",
    "# (1) removing outliers from fare_amount column with the help of violinplot\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.violinplot(df[\"fare_amount\"], color='#008000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31cea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['fare_amount']<=80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "sns.violinplot(df[\"fare_amount\"], color='#008000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a905cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['fare_amount']<=59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5522891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "sns.violinplot(df[\"fare_amount\"], color='#008000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc7100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d143a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) removing outliers from H_Distance column with the help of violinplot\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.violinplot(df[\"H_Distance\"], color='#008000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['H_Distance']<1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2790253",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "sns.violinplot(df[\"H_Distance\"], color='#008000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f93fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['H_Distance']<100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a8b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "sns.violinplot(df[\"H_Distance\"], color='#008000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb3d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['H_Distance']<23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d23ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "sns.violinplot(df[\"H_Distance\"], color='#008000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccad4f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06c7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) removing outliers from Fare_Amount_Per_KM column with the help of violinplot\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.violinplot(df[\"Fare_Amount_Per_KM\"], color='#008000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860eae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Fare_Amount_Per_KM']<1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fde276",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "sns.violinplot(df[\"Fare_Amount_Per_KM\"], color='#008000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d21969",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Fare_Amount_Per_KM']<50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a505a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "sns.violinplot(df[\"Fare_Amount_Per_KM\"], color='#008000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c72c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Fare_Amount_Per_KM']<15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "sns.violinplot(df[\"Fare_Amount_Per_KM\"], color='#008000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b09e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting graph between 'fare_amount' and 'H_Distance'\n",
    "temp_df = df[['fare_amount','H_Distance']].sort_values('H_Distance')\n",
    "plt.plot(temp_df['H_Distance'],temp_df['fare_amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad87c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting graph between 'fare_amount' and 'Year'\n",
    "# It shows how fare_amount increasing as year incresing\n",
    "temp_df = df[['fare_amount','Year']].sort_values('Year')\n",
    "plt.plot(temp_df['Year'],temp_df['fare_amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.index,df.fare_amount)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35926bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.H_Distance,df.fare_amount)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.fare_amount,df.passenger_count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.Fare_Amount_Per_KM,df.fare_amount)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9126812",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.index,df.fare_amount)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b044afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of the coefficient of determination of the prediction.\n",
    "\n",
    "X = df[['H_Distance','Year','Month',\"Date\",'Day of Week','Hour']]\n",
    "y = df['fare_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6197b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "# printing the length of X and y\n",
    "print(len(X),len(y))\n",
    "for index, val in enumerate(zip(X, y)):\n",
    "    arr.append([val[0], val[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef17486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db09c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Linear.fit(np.array(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS.score(np.array(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d75a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6cdaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f2f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30d87ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca5941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c688f2bb",
   "metadata": {},
   "source": [
    "# 2.DATA SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9be937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating another copy df dataset\n",
    "df_1 = pd.DataFrame(df.copy())\n",
    "\n",
    "# Before scaling\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are doing data scaling because sacling of data makes it easy to train model\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bc86d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am doing scaling of df_1 dataframe\n",
    "for col in ['passenger_count','fare_amount','H_Distance','Fare_Amount_Per_KM','Year','Month','Date','Day of Week','Hour']:\n",
    "    df_1[col] = scaler.fit_transform(df_1[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e60c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After scaling\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351eaa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting df_1 dataframe according H_Distance\n",
    "df_1 = df_1.sort_values('H_Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c0db40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c7aa93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2b4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b25883f",
   "metadata": {},
   "source": [
    "# 3.Building a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32169fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data_processing_Visualization class for adding H_Distance,splitting pickup_datetime into Year,day.. and also for removing outliers  \n",
    "\n",
    "class Data_processing_Visualization(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,train, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,train):\n",
    "        \n",
    "        #         ============================================================\n",
    "        train = train[['key', 'fare_amount','pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n",
    "       'dropoff_longitude', 'dropoff_latitude', 'passenger_count']]\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "        train = train.dropna()\n",
    "        \n",
    "#         ============================================================\n",
    "        train['key'] = pd.to_datetime(train['key'])\n",
    "        train['pickup_datetime']  = pd.to_datetime(train['pickup_datetime'])\n",
    "        \n",
    "#         ==============================================================\n",
    "        data = [train]\n",
    "        for i in data:\n",
    "            i['Year'] = i['pickup_datetime'].dt.year\n",
    "            i['Month'] = i['pickup_datetime'].dt.month\n",
    "            i['Date'] = i['pickup_datetime'].dt.day\n",
    "            i['Day of Week'] = i['pickup_datetime'].dt.dayofweek\n",
    "            i['Hour'] = i['pickup_datetime'].dt.hour\n",
    "            \n",
    "#        =============================================================\n",
    "        df = pd.DataFrame(train, columns=['pickup_datetime','pickup_longitude', 'pickup_latitude', 'dropoff_longitude', \n",
    "                'dropoff_latitude','passenger_count','fare_amount','Year', 'Month', 'Date','Day of Week', 'Hour'])\n",
    "    \n",
    "#     ===============================================================\n",
    "        df = df[df['pickup_latitude']>=-90]\n",
    "        df = df[df['pickup_latitude']<=90]\n",
    "        df = df[df['dropoff_latitude']>=-90]\n",
    "        df = df[df['dropoff_latitude']<=90]\n",
    "        df = df[df['pickup_longitude']>=-180]\n",
    "        df = df[df['pickup_longitude']<=180]\n",
    "        df = df[df['dropoff_longitude']>=-180]\n",
    "        df = df[df['dropoff_longitude']<=180]\n",
    "        lat1 ='pickup_latitude'\n",
    "        long1 = 'pickup_longitude'\n",
    "        lat2 = 'dropoff_latitude'\n",
    "        long2 = 'dropoff_longitude'\n",
    "        \n",
    "        data = [df]\n",
    "        for i in data:\n",
    "            R = 6371  #radius of earth in kilometers\n",
    "            phi1 = np.radians(i[lat1])\n",
    "            phi2 = np.radians(i[lat2])\n",
    "\n",
    "            delta_phi = np.radians(i[lat2]-i[lat1])\n",
    "            delta_lambda = np.radians(i[long2]-i[long1])\n",
    "\n",
    "            #a = sin²((φB - φA)/2) + cos φA . cos φB . sin²((λB - λA)/2)\n",
    "            a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0) ** 2\n",
    "\n",
    "            #c = 2 * atan2( √a, √(1−a) )\n",
    "            c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "\n",
    "            #d = R*c\n",
    "            d = (R * c) #in kilometers\n",
    "            i['H_Distance'] = d\n",
    "#          ==============================================================\n",
    "        del df['pickup_datetime']\n",
    "        del df['pickup_longitude']\n",
    "        del df['pickup_latitude']\n",
    "        del df['dropoff_longitude']\n",
    "        del df['dropoff_latitude']\n",
    "        df = df[df['H_Distance']>0]\n",
    "        df = df[df['fare_amount']>0]\n",
    "        df = df[df['passenger_count']>0]\n",
    "        \n",
    "        \n",
    "#         ========================================================\n",
    "        col1 = 'fare_amount'\n",
    "        col2 = 'H_Distance'\n",
    "        data = [df]\n",
    "        for i in data:\n",
    "            ans = i[col1]/i[col2]\n",
    "            i['Fare_Amount_Per_KM'] = ans\n",
    "#         ====================================================\n",
    "        df = df[df['H_Distance']<23]\n",
    "        df = df[df['Fare_Amount_Per_KM']<15]\n",
    "        df = df[df['fare_amount']<=59]\n",
    "        \n",
    "        return df\n",
    "#         ========================================================\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = pd.read_csv(\"Desktop/DATASET/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222fda92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just checking Data_processing_Visualization class working properly or not \n",
    "x = Data_processing_Visualization()\n",
    "x.fit_transform(train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another class for scaling\n",
    "class Data_Scaling(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,train, y=None):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        scaler = StandardScaler()\n",
    "        for col in ['passenger_count','fare_amount','H_Distance','Fare_Amount_Per_KM','Year','Month','Date','Day of Week','Hour']:\n",
    "            df[col] = scaler.fit_transform(df[[col]])\n",
    "        return df    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c1db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Pipline = Pipeline([('Data_1',Data_processing_Visualization()),('Data_2',Data_Scaling())])\n",
    "ans = Data_Pipline.fit_transform(train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d34ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_1 dataframe After removing outliers and Scaling\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93300f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbcf6d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce55374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08700337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a22747e",
   "metadata": {},
   "source": [
    "# 4.Use Of Validation Set and Cross Validation Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf339bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are fixing value of k for k_cross folding validation\n",
    "n_splits = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation_set = df_1[['Year','passenger_count','fare_amount','H_Distance','Fare_Amount_Per_KM']].copy()\n",
    "Validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f52cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the DataFrame rows\n",
    "Validation_set = Validation_set.sample(frac = 1)\n",
    "Validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking shape of Validation_set\n",
    "Validation_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d730e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Method  for validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148e347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  k-fold cross validation algorithm from scratch to evaluate our model and choose hyper-parameters using libraries\n",
    "\n",
    "# Here i am declaring some array for futher calculation to store Actual value of fare_amount,\n",
    "# Predicted value of fare_amount and H_Distance\n",
    "\n",
    "\n",
    "Actual_value_1 = np.array([])\n",
    "Predicted_Value_1 = np.array([])\n",
    "Distance_value_1 = np.array([])\n",
    "\n",
    "total_range = len(Validation_set)//n_splits\n",
    "# total_range length of testing data and remaining is training data\n",
    "\n",
    "Index = 0\n",
    "Score = 0\n",
    "\n",
    "\n",
    "while Index<n_splits:    \n",
    "    \n",
    "    test_data = Validation_set[Index*total_range:Index*total_range+total_range:]\n",
    "    notcommonentries = Validation_set[~Validation_set.isin(test_data)]\n",
    "    \n",
    "    train_data = notcommonentries.dropna()\n",
    "    Fare_amount_train = train_data['fare_amount']\n",
    "    Fare_amount_test = test_data['fare_amount']\n",
    "    \n",
    "#     print(Fare_amount_train)   \n",
    "#     print(Fare_amount_test)\n",
    "\n",
    "    del train_data['fare_amount']\n",
    "    del test_data['fare_amount']\n",
    "    \n",
    "    data_fit = LinearRegression()\n",
    "    data_fit.fit(train_data, Fare_amount_train)\n",
    "    test_prediction = data_fit.predict(test_data)\n",
    "    \n",
    "    \n",
    "    df_actual_pred = pd.DataFrame({'Actual': Fare_amount_test.squeeze(), 'Predicted': test_prediction.squeeze()})\n",
    "    Actual= df_actual_pred['Actual']\n",
    "    predict = df_actual_pred['Predicted']\n",
    "    distance = test_data['H_Distance']\n",
    "\n",
    "    Actual_value_1 = np.append (Actual_value_1, Actual)\n",
    "    Predicted_Value_1 = np.append (Predicted_Value_1, predict)\n",
    "    Distance_value_1 = np.append (Distance_value_1, distance)\n",
    "    \n",
    "    Index+=1\n",
    "#     print(Distance_value_1)\n",
    "#     print(Actual_value_1)\n",
    "#     print(Predicted_Value_1)\n",
    "    \n",
    "      \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d9c1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Final_df_1 =  pd.DataFrame({\n",
    "    'Distance':Distance_value_1,\n",
    "    'Actual_Value': Actual_value_1,\n",
    "    'Predicted_Value':Predicted_Value_1\n",
    "})\n",
    "Final_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of mean_squared_error \n",
    "MSE=np.array((Final_df_1['Actual_Value']-Final_df_1['Predicted_Value'])**2)\n",
    "print('mean_squared_error is',np.mean(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a0f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=[12,10]\n",
    "plt.scatter(Final_df_1['Distance'],Final_df_1['Actual_Value'])\n",
    "plt.plot(Final_df_1['Distance'],Final_df_1['Predicted_Value'],color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3dae8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149155a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a18dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second Method for validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049dfed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca076fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  k-fold cross validation algorithm from scratch to evaluate our model and choose hyper-parameters using libraries\n",
    "\n",
    "# Here i am declaring some array for futher calculation to store Actual value of fare_amount,\n",
    "# Predicted value of fare_amount and H_Distance\n",
    "\n",
    "\n",
    "Actual_value_2 = np.array([])\n",
    "Predicted_Value_2 = np.array([])\n",
    "Distance_value_2 = np.array([])\n",
    "\n",
    "total_range = len(Validation_set)//n_splits\n",
    "# total_range length of testing data and remaining is training data\n",
    "\n",
    "Index = 0\n",
    "Score = 0\n",
    "\n",
    "while Index<n_splits:    \n",
    "    test_data = Validation_set[Index*total_range:Index*total_range+total_range:]\n",
    "    notcommonentries = Validation_set[~Validation_set.isin(test_data)]\n",
    "    \n",
    "    train_data = notcommonentries.dropna()\n",
    "    Fare_amount_train = train_data['fare_amount']\n",
    "    Fare_amount_test = test_data['fare_amount']\n",
    "    \n",
    "#     print(Fare_amount_train)   \n",
    "#     print(Fare_amount_test)\n",
    "\n",
    "    del train_data['fare_amount']\n",
    "    del test_data['fare_amount']\n",
    "    \n",
    "    A_MAT = train_data.to_numpy()\n",
    "    pseudo_inverse = np.linalg.pinv(A_MAT)\n",
    "    ANS = np.dot(pseudo_inverse,Fare_amount_train)\n",
    "    Final_ANS = np.dot(test_data,ANS)\n",
    "    \n",
    "\n",
    "    Actual_value_2 = np.append (Actual_value_2, Actual)\n",
    "    Predicted_Value_2 = np.append (Predicted_Value_2, predict)\n",
    "    Distance_value_2 = np.append (Distance_value_2, distance)\n",
    "    \n",
    "    Index+=1\n",
    "    \n",
    "#     print(Distance_value_2)\n",
    "#     print(Actual_value_2)\n",
    "#     print(Predicted_Value_2)    \n",
    "    \n",
    "# ============================================    \n",
    "    \n",
    "#     A_MAT = train_data.to_numpy()\n",
    "#     A_MAT_Trans = np.transpose(A_MAT)\n",
    "#     Final_ANS = np.dot(A_MAT_Trans,A_MAT)\n",
    "#     INV = np.linalg.inv(Final_ANS)\n",
    "#     ans  = np.dot(INV,A_MAT_Trans)\n",
    "#     ans_1 = np.dot(ans,Fare_amount_train)\n",
    "#     Final_ANS = np.dot(test_data,np.dot(np.dot(np.linalg.inv(Final_ANS),A_MAT_Trans),Fare_amount_train))\n",
    "\n",
    "\n",
    "\n",
    "# ===================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_df_2 =  pd.DataFrame({\n",
    "    'Distance':Distance_value_2,\n",
    "    'Actual_Value': Actual_value_2,\n",
    "    'Predicted_Value':Predicted_Value_2\n",
    "})\n",
    "Final_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45edce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of mean_squared_error \n",
    "MSE=np.array((Final_df_2['Actual_Value']-Final_df_2['Predicted_Value'])**2)\n",
    "print('mean_squared_error is',np.mean(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=[12,10]\n",
    "plt.scatter(Final_df_2['Distance'],Final_df_2['Actual_Value'])\n",
    "plt.plot(Final_df_2['Distance'],Final_df_2['Predicted_Value'],color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47af6082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50213b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e9113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54c699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98de404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f60f5bcc",
   "metadata": {},
   "source": [
    "# 5.Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a12ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into X_train, X_test, y_train, y_test with the help of train_test_split libraries\n",
    "train_data_5 = Validation_set.copy()\n",
    "y = train_data_5['fare_amount']\n",
    "X = train_data_5.drop(['fare_amount'],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb56653d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253daac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f81e843d",
   "metadata": {},
   "source": [
    "**Matrix Based:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c7022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating mean_squared_error of a model using Matrix Based regressors\n",
    "\n",
    "linear = linear_model.LinearRegression()\n",
    "linear.fit(X_train, y_train)\n",
    "ANS_Linear = linear.predict(X_test)\n",
    "print('mean_squared_error of a model using Matrix Based regressors',mean_squared_error(ANS_Linear, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec43abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d299dab",
   "metadata": {},
   "source": [
    "**Optimization Based**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating mean_squared_error of a model using Optimization Based regressors\n",
    "\n",
    "sgd = SGDRegressor(max_iter=800000,eta0=0.000001)\n",
    "sgd.fit(X_train,y_train)\n",
    "ANS_sgd = sgd.predict(X_test)\n",
    "print('mean_squared_error of a model using Matrix Based regressors',mean_squared_error(ANS_sgd, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a75808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "882d3760",
   "metadata": {},
   "source": [
    "**Non-parametric Based**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c85c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating mean_squared_error of a model using Non-parametric based regressors\n",
    "\n",
    "neighbour = KNeighborsRegressor(n_neighbors=2)\n",
    "neighbour.fit(X_train,y_train)\n",
    "ANS_neighbour = neighbour.predict(X_test)\n",
    "print('mean_squared_error of a model using Matrix Based regressors',mean_squared_error(ANS_neighbour, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf4f59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1d8058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d0528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c19dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f66169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fd5a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a34e8fe4",
   "metadata": {},
   "source": [
    "# Part 2: Life Expectancy (WHO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95efb997",
   "metadata": {},
   "source": [
    "# 1.Data Cleaning and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc33ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data set in train dataset\n",
    "train = pd.read_csv(\"Desktop/DATASET/Life_Expectancy_Data.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db24ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2bb992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list_2 to store names of columns of train dataset\n",
    "list_2 = ['Life expectancy ', 'Adult Mortality',\n",
    "       'infant deaths', 'Alcohol', 'percentage expenditure', 'Hepatitis B',\n",
    "       'Measles ', ' BMI ', 'under-five deaths ', 'Polio', 'Total expenditure',\n",
    "       'Diphtheria ', ' HIV/AIDS', 'GDP', 'Population',\n",
    "       ' thinness  1-19 years', ' thinness 5-9 years',\n",
    "       'Income composition of resources', 'Schooling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25121209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are ploting multiple histogram for better understanding\n",
    "\n",
    "figure, axis = plt.subplots(19,1, figsize=(20,80))\n",
    "for i in range(len(list_2)):\n",
    "    axis[i].hist(train[list_2[i]], bins=100)\n",
    "    axis[i].set_title(list_2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb9621",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b478cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking whether train dataset have NAN value or not\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0204ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping raws which has NAN values\n",
    "train.dropna(axis=0,how='any',thresh=None, subset=None, inplace=True)\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e4a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are droping coutry column from train dataset\n",
    "train = train.drop(['Country'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a268f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers from train dataset\n",
    "train = train[train[' BMI ']<=60]\n",
    "train = train[train[' BMI ']>0]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce04399",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a0c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = pd.read_csv(\"Desktop/DATASET/Life_Expectancy_Data.csv\")\n",
    "train_1.dropna(axis=0,how='any',thresh=None, subset=None, inplace=True)\n",
    "\n",
    "countries_names = train_1['Country']\n",
    "\n",
    "\n",
    "nation = {}\n",
    "\n",
    "for country in pycountry.countries:\n",
    "   nation[country.name] = country.alpha_3\n",
    "\n",
    "train_1['3 Code'] = [nation.get(country, 'Unknown code') for country in countries_names]\n",
    "\n",
    "\n",
    "\n",
    "df = train_1.query(\"Year == 2007\")\n",
    "# df = px.data.gapminder().query(\"year == 2007\")\n",
    "df.describe()\n",
    "fig = px.choropleth(df, locations=\"3 Code\",\n",
    "                    color=\"Life expectancy \",  # lifeExp is a column of gapminder\n",
    "                    hover_name=\"Country\",  # column to add to hover information\n",
    "                    color_continuous_scale=px.colors.sequential.Plasma\n",
    "                    )\n",
    "fig.show()\n",
    "\n",
    "\n",
    "df = train_1.query(\"Year == 2007\")\n",
    "# df = px.data.gapminder().query(\"year == 2007\")\n",
    "df.describe()\n",
    "fig = px.choropleth(df, locations=\"3 Code\",\n",
    "                    color=\" BMI \",  # lifeExp is a column of gapminder\n",
    "                    hover_name=\"Country\",  # column to add to hover information\n",
    "                    color_continuous_scale=px.colors.sequential.Plasma\n",
    "                    )\n",
    "fig.show()\n",
    "\n",
    "import plotly.express as px\n",
    "df = train_1.query(\"Year == 2007\")\n",
    "# df = px.data.gapminder().query(\"year == 2007\")\n",
    "df.describe()\n",
    "fig = px.choropleth(df, locations=\"3 Code\",\n",
    "                    color='Alcohol',  # lifeExp is a column of gapminder\n",
    "                    hover_name=\"Country\",  # column to add to hover information\n",
    "                    color_continuous_scale=px.colors.sequential.Plasma\n",
    "                    )\n",
    "fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e03f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that in south america life expantancy nearly all similar and same goes for middle asia\n",
    "# and also intake of alcohol in russia is more in asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe0f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "sns.violinplot(train['Life expectancy '], color='#008000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a146102",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "sns.violinplot(train[' BMI '], color='#008000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cea2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "sns.violinplot(train['Adult Mortality'], color='#008000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a391c2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train['Alcohol'], train['Life expectancy '])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e747bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train['percentage expenditure'], train['Life expectancy '])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd03bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train['Hepatitis B'], train['Life expectancy '])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68214d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train[' BMI '], train['Life expectancy '])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f06e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train['GDP'], train['Life expectancy '])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea079fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train['Income composition of resources'], train['Life expectancy '])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train['Schooling'],train['under-five deaths '])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8808da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train[' HIV/AIDS'],train['Life expectancy '])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac7ce5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae36449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e1706d9",
   "metadata": {},
   "source": [
    "# 2. Distribution analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b54e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ac7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2 = pd.read_csv(\"Desktop/DATASET/Life_Expectancy_Data.csv\")\n",
    "train_2.dropna(axis=0,how='any',thresh=None, subset=None, inplace=True)\n",
    "train_2 = train_2.drop(['Country'],axis=1)\n",
    "train_2 = train_2.drop(['Status'],axis=1)\n",
    "\n",
    "for i in train_2.columns:\n",
    "    \n",
    "    x , y = kstest(train_2[i], 'norm') \n",
    "    \n",
    "#     print(x)\n",
    "    print('P-Value of',i,'is',y)\n",
    "\n",
    "# below answer is showing that distribution of all the columns are different from normal distribution   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932fd85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2\n",
    "# Source\n",
    "# https://towardsdatascience.com/comparing-sample-distributions-with-the-kolmogorov-smirnov-ks-test-a2292ad6fee5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4144493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_norm(sample):\n",
    "    # Sorts the sample\n",
    "    sample.sort()\n",
    "    # Evaluates the KS statistic\n",
    "    D_ks = [] # KS Statistic list\n",
    "    for x in sample:\n",
    "        cdf_normal = stats.norm.cdf(x = x, loc = 0, scale = 1)\n",
    "        cdf_sample = cdf(sample = sample, x  = x)\n",
    "        D_ks.append(abs(cdf_normal - cdf_sample))\n",
    "    ks_stat = max(D_ks)\n",
    "    # Calculates the P-Value based on the two-sided test\n",
    "    # The P-Value comes from the KS Distribution Survival Function (SF = 1-CDF)\n",
    "    p_value = stats.kstwo.sf(ks_stat, len(sample))\n",
    "    return {\"ks_stat\": ks_stat, \"p_value\" : p_value}\n",
    "\n",
    "\n",
    "# https://towardsdatascience.com/comparing-sample-distributions-with-the-kolmogorov-smirnov-ks-test-a2292ad6fee5\n",
    "def cdf(sample, x, sort = False):\n",
    "    # Sorts the sample, if unsorted\n",
    "    if sort:\n",
    "        sample.sort()\n",
    "    # Counts how many observations are below x\n",
    "    cdf = sum(sample <= x)\n",
    "    # Divides by the total number of observations\n",
    "    cdf = cdf / len(sample)\n",
    "    return cdf\n",
    "\n",
    "def ks_2samp(sample1, sample2):\n",
    "    # Gets all observations\n",
    "    observations = np.concatenate((sample1, sample2))\n",
    "    observations.sort()\n",
    "    # Sorts the samples\n",
    "#     sample1.sort()\n",
    "#     sample2.sort()\n",
    "    # Evaluates the KS statistic\n",
    "    D_ks = [] # KS Statistic list\n",
    "    for x in observations:\n",
    "        cdf_sample1 = cdf(sample = sample1, x  = x)\n",
    "        cdf_sample2 = cdf(sample = sample2, x  = x)\n",
    "        D_ks.append(abs(cdf_sample1 - cdf_sample2))\n",
    "    ks_stat = max(D_ks)\n",
    "    # Calculates the P-Value based on the two-sided test\n",
    "    # The P-Value comes from the KS Distribution Survival Function (SF = 1-CDF)\n",
    "    m, n = float(len(sample1)), float(len(sample2))\n",
    "    en = m * n / (m + n)\n",
    "    p_value = stats.kstwo.sf(ks_stat, np.round(en))\n",
    "    return {\"ks_stat\": ks_stat, \"p_value\" : p_value}\n",
    "\n",
    "\n",
    "# Evaluates all possible combinations.\n",
    "# We want to know if the distributions are identical, so we cannot standardize them\n",
    "# [, , , ,\n",
    "#        'Income composition of resources', 'Schooling']\n",
    "sets = [train_2['Adult Mortality'], train_2[' BMI '], train_2[' HIV/AIDS'], train_2['GDP']]\n",
    "names = ['Adult Mortality', ' BMI ', ' HIV/AIDS', 'GDP',]\n",
    "ks_scores = {}\n",
    "for _ in range(len(names)):\n",
    "    name1 = names.pop(0)\n",
    "    sample1 = sets.pop(0)\n",
    "#     print(name1,sample1)\n",
    "    for name2, sample2 in zip(names, sets):\n",
    "        key1 = name1 + \"_\" +  name2\n",
    "        key2 = name2 + \"_\" +  name1\n",
    "        ks = ks_2samp(sample1, sample2)\n",
    "        ks_scores[key1] = ks\n",
    "        ks_scores[key2] = ks\n",
    "# Prints the results\n",
    "\n",
    "print(f\"Adult Mortality vs BMI: ks = {ks_scores['Adult Mortality_ BMI ']['ks_stat']:.4f} (p-value = {ks_scores['Adult Mortality_ BMI ']['p_value']:.3e}, are equal = {ks_scores['Adult Mortality_ BMI ']['p_value'] > 0.05})\")\n",
    "print(f\"Adult Mortality vs HIV/AIDS: ks = {ks_scores['Adult Mortality_ HIV/AIDS']['ks_stat']:.4f} (p-value = {ks_scores['Adult Mortality_ HIV/AIDS']['p_value']:.3e}, are equal = {ks_scores['Adult Mortality_ HIV/AIDS']['p_value'] > 0.05})\")\n",
    "print(f\"Adult Mortality vs GDP: ks = {ks_scores['Adult Mortality_GDP']['ks_stat']:.4f} (p-value = {ks_scores['Adult Mortality_GDP']['p_value']:.3e}, are equal = {ks_scores['Adult Mortality_GDP']['p_value'] > 0.05})\")\n",
    "print(f\"BMI vs HIV/AIDS: ks = {ks_scores[' BMI _ HIV/AIDS']['ks_stat']:.4f} (p-value = {ks_scores[' BMI _ HIV/AIDS']['p_value']:.3e}, are equal = {ks_scores[' BMI _ HIV/AIDS']['p_value'] > 0.05})\")\n",
    "print(f\"BMI vs GDP: ks = {ks_scores[' BMI _GDP']['ks_stat']:.4f} (p-value = {ks_scores[' BMI _GDP']['p_value']:.3e}, are equal = {ks_scores[' BMI _GDP']['p_value'] > 0.05})\")\n",
    "print(f\"HIV/AIDS vs GDP: ks = {ks_scores[' HIV/AIDS_GDP']['ks_stat']:.4f} (p-value = {ks_scores[' HIV/AIDS_GDP']['p_value']:.3e}, are equal = {ks_scores[' HIV/AIDS_GDP']['p_value'] > 0.05})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d88846",
   "metadata": {},
   "source": [
    "# 3. Data Scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are doing data scaling because sacling of data makes it easy to train model\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = train_1.drop(['Country'],axis=1)\n",
    "train_1 = train_1.drop(['Status'],axis=1)\n",
    "train_1 = train_1.drop(['3 Code'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f93b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am doing scaling of df_1 dataframe\n",
    "for col in train_1.columns:\n",
    "    train_1[col] = scaler.fit_transform(train_1[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3cc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after scaling\n",
    "train_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05e6308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15b21cec",
   "metadata": {},
   "source": [
    "# 4. Building a Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data_processing_Visualization class for adding H_Distance,splitting pickup_datetime into Year,day.. and also for removing outliers  \n",
    "class Data_processing_Visualization(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,train_3, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,train_3):\n",
    "        train_3.dropna(axis=0,how='any',thresh=None, subset=None, inplace=True)\n",
    "        \n",
    "        train_3 = train_3[train_3[' BMI ']<=60]\n",
    "        train_3 = train_3[train_3[' BMI ']>0]\n",
    "        train_3 = train_3.drop(['Country'],axis=1)\n",
    "        train_3 = train_3.drop(['Status'],axis=1)\n",
    "      \n",
    "        return train_3\n",
    "#         ========================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0677be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another class for scaling\n",
    "class Data_Scaling(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,train_3, y=None):\n",
    "        return self\n",
    "    def transform(self,train_3):\n",
    "        scaler = StandardScaler()\n",
    "        for col in train_3.columns:\n",
    "            train_3[col] = scaler.fit_transform(train_3[[col]])\n",
    "        return train_3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bbeb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_3 = pd.read_csv(\"Desktop/DATASET/Life_Expectancy_Data.csv\")\n",
    "Data_Pipline = Pipeline([('Data_1',Data_processing_Visualization()),('Data_2',Data_Scaling())])\n",
    "ans = Data_Pipline.fit_transform(train_3)\n",
    "ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d783e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "481e30fc",
   "metadata": {},
   "source": [
    "# 5. Use of Validation Set and Cross Validation Approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ae7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Method  for validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a94fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are fixing value of k for k_cross folding validation\n",
    "n_splits = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07d1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  k-fold cross validation algorithm from scratch to evaluate our model and choose hyper-parameters using libraries\n",
    "\n",
    "# Here i am declaring some array for futher calculation to store Actual value of Life expectancy and\n",
    "# Predicted value of Life expectancy \n",
    "\n",
    "Actual_value_1 = np.array([])\n",
    "Predicted_Value_1 = np.array([])\n",
    "\n",
    "total_range = len(train)//n_splits\n",
    "Index = 0\n",
    "Score = 0\n",
    "\n",
    "\n",
    "while Index<n_splits:    \n",
    "    test_data = train_1[Index*total_range:Index*total_range+total_range:]\n",
    "    notcommonseries = train_1[~train_1.isin(test_data)]\n",
    "    \n",
    "    train_data = notcommonseries.dropna()\n",
    "    Life_Expectancy_train = train_data['Life expectancy ']\n",
    "    del train_data['Life expectancy ']\n",
    "    data_fit = LinearRegression()\n",
    "    data_fit.fit(train_data, Life_Expectancy_train)\n",
    "    Life_Expectancy_test = test_data['Life expectancy ']\n",
    "\n",
    "    del test_data['Life expectancy ']\n",
    "    test_prediction = data_fit.predict(test_data)\n",
    "    df_actual_pred = pd.DataFrame({'Actual': Life_Expectancy_test.squeeze(), 'Predicted': test_prediction.squeeze()})\n",
    "    Actual= df_actual_pred['Actual']\n",
    "    predict = df_actual_pred['Predicted']\n",
    "\n",
    "    Actual_value_1 = np.append (Actual_value_1, Actual)\n",
    "    Predicted_Value_1 = np.append (Predicted_Value_1, predict)\n",
    "\n",
    "#     print(Distance_value)\n",
    "#     print(Actual_value)\n",
    "#     print(Predicted_Value)\n",
    "    \n",
    "    Index+=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_df_1 =  pd.DataFrame({\n",
    "    'Actual_Value': Actual_value_1,\n",
    "    'Predicted_Value':Predicted_Value_1\n",
    "})\n",
    "Final_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6128c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of mean_squared_error \n",
    "MSE=np.array((Final_df_1['Actual_Value']-Final_df_1['Predicted_Value'])**2)\n",
    "print('mean_squared_error is',np.mean(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8325ab47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third Method  for validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290936f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Actual_value_2 = np.array([])\n",
    "Predicted_Value_2 = np.array([])\n",
    "\n",
    "\n",
    "total_range = len(train_1)//n_splits\n",
    "\n",
    "Index = 0\n",
    "Score = 0\n",
    "while Index<n_splits:    \n",
    "    test_data = train_1[Index*total_range:Index*total_range+total_range:]\n",
    "    notcommonseries = train_1[~train_1.isin(test_data)]\n",
    "\n",
    "    train_data = notcommonseries.dropna()\n",
    "    Life_expectancy_train = train_data['Life expectancy ']\n",
    "    del train_data['Life expectancy ']\n",
    "    Life_expectancy_test = test_data['Life expectancy ']\n",
    "    del test_data['Life expectancy ']\n",
    "    A_MAT = train_data.to_numpy()\n",
    "    pseudo_inverse = np.linalg.pinv(A_MAT)\n",
    "\n",
    "    Final_ANS = np.dot(pseudo_inverse,Life_expectancy_train)\n",
    "    Final_ANS = np.dot(test_data,Final_ANS)\n",
    "\n",
    "\n",
    "    Actual_value_2 = np.append (Actual_value_2, Life_expectancy_test)\n",
    "    Predicted_Value_2 = np.append (Predicted_Value_2, Final_ANS)\n",
    "\n",
    "   \n",
    "    Index+=1  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e233de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Final_df_2 =  pd.DataFrame({\n",
    "    'Actual_Value': Actual_value_2,\n",
    "    'Predicted_Value':Predicted_Value_2\n",
    "})\n",
    "Final_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of mean_squared_error \n",
    "MSE=np.array((Final_df_2['Actual_Value']-Final_df_2['Predicted_Value'])**2)\n",
    "print('mean_squared_error is',np.mean(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe285b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88250a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b7069c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e02c0ed4",
   "metadata": {},
   "source": [
    "# 6. Feature Selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf1b05",
   "metadata": {},
   "source": [
    "**Feature Selection using lasso method regulizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56952f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.yourdatateacher.com/2021/05/05/feature-selection-in-machine-learning-using-lasso-regression/#:~:text=How%20can%20we%20use%20it,its%20coefficient%20equal%20to%200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44276bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_1 = ['Year', 'Adult Mortality', 'infant deaths',\n",
    "       'Alcohol', 'percentage expenditure', 'Hepatitis B', 'Measles ', ' BMI ',\n",
    "       'under-five deaths ', 'Polio', 'Total expenditure', 'Diphtheria ',\n",
    "       ' HIV/AIDS', 'GDP', 'Population', ' thinness  1-19 years',\n",
    "       ' thinness 5-9 years', 'Income composition of resources', 'Schooling']\n",
    "X=train_1.drop(['Life expectancy '],axis=1)\n",
    "y=train_1[['Life expectancy ']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c2d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "                     ('scaler',StandardScaler()),\n",
    "                     ('model',Lasso())\n",
    "])\n",
    "Val = GridSearchCV(pipeline,\n",
    "                      {'model__alpha':np.arange(0.1,10,0.1)},\n",
    "                      cv = 5, scoring=\"neg_mean_squared_error\",verbose=0\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd1f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e440a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Val.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "Val.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = Val.best_estimator_.named_steps['model'].coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aec4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = np.abs(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d07afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_1.columns\n",
    "w = np.array(list_1)[importance > 0]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6611a1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eebb4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae7c15f6",
   "metadata": {},
   "source": [
    "**Feature Selection using OLS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b90786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats():\n",
    "#     x = train_1[list_1]\n",
    "    results = sm.OLS(y_train,X_train).fit()\n",
    "    print(results.summary())\n",
    "get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109e205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52bce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adfc9d1d",
   "metadata": {},
   "source": [
    "**Feature Selection using Scikit learn library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e4d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs = SequentialFeatureSelector(Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457daec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.fit(X_train, y_train)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d31f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189fc570",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = train_1[['Adult Mortality', ' BMI ', ' HIV/AIDS', 'GDP',\n",
    "       ' thinness 5-9 years', 'Income composition of resources',\n",
    "       'Schooling','Life expectancy ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600b0855",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dadac7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec58429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f855f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error Calculation after feature selection(LinearRegression)\n",
    "# I am using feature to error calculation which are get from lasso method regulizer \n",
    "# because it is giving less feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41244a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_1['Life expectancy ']\n",
    "X = train_1.drop(['Life expectancy '],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeeccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e43fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7563774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de7b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS_Linear = Linear.predict(X_test)\n",
    "mean_squared_error(ANS_Linear, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decdfc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9487e0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e56f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
